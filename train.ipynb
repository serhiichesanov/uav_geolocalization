{
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "bgoOgTg19BMp",
    "Dvpl9mtI8mN1",
    "EZWV7p-J6ouM"
   ],
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "sourceId": 8016182,
     "sourceType": "datasetVersion",
     "datasetId": 4722935
    },
    {
     "sourceId": 8197477,
     "sourceType": "datasetVersion",
     "datasetId": 4855778
    }
   ],
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": "pip install pytorch_metric_learning",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-26T12:26:10.120299Z",
     "iopub.execute_input": "2024-04-26T12:26:10.120597Z",
     "iopub.status.idle": "2024-04-26T12:26:23.522684Z",
     "shell.execute_reply.started": "2024-04-26T12:26:10.120570Z",
     "shell.execute_reply": "2024-04-26T12:26:23.521585Z"
    },
    "trusted": true
   },
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "text": "Collecting pytorch_metric_learning\n  Downloading pytorch_metric_learning-2.5.0-py3-none-any.whl.metadata (17 kB)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from pytorch_metric_learning) (1.26.4)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from pytorch_metric_learning) (1.2.2)\nRequirement already satisfied: torch>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from pytorch_metric_learning) (2.1.2)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from pytorch_metric_learning) (4.66.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->pytorch_metric_learning) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->pytorch_metric_learning) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->pytorch_metric_learning) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->pytorch_metric_learning) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->pytorch_metric_learning) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->pytorch_metric_learning) (2024.3.0)\nRequirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->pytorch_metric_learning) (1.11.4)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->pytorch_metric_learning) (1.3.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->pytorch_metric_learning) (3.2.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.6.0->pytorch_metric_learning) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.6.0->pytorch_metric_learning) (1.3.0)\nDownloading pytorch_metric_learning-2.5.0-py3-none-any.whl (119 kB)\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m119.1/119.1 kB\u001B[0m \u001B[31m4.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\n\u001B[?25hInstalling collected packages: pytorch_metric_learning\nSuccessfully installed pytorch_metric_learning-2.5.0\nNote: you may need to restart the kernel to use updated packages.\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "import os\nimport math\nimport timm\nimport random\nimport torch\nimport wandb\nimport cv2 as cv\nimport pandas as pd\nimport numpy as np\nimport pickle\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.nn import Module\nfrom torchvision.models import vgg16, VGG16_Weights, ResNet50_Weights\nfrom torchvision import transforms\nimport torchvision\nfrom PIL import Image\nfrom enum import Enum\nfrom pytorch_metric_learning import miners, losses\nimport albumentations as A\n#from torchgeo import datasets",
   "metadata": {
    "id": "JjnNRhhKmtpc",
    "ExecuteTime": {
     "end_time": "2024-04-02T19:36:20.467330800Z",
     "start_time": "2024-04-02T19:36:20.245334900Z"
    },
    "execution": {
     "iopub.status.busy": "2024-04-26T12:27:03.144118Z",
     "iopub.execute_input": "2024-04-26T12:27:03.144941Z",
     "iopub.status.idle": "2024-04-26T12:27:03.151550Z",
     "shell.execute_reply.started": "2024-04-26T12:27:03.144906Z",
     "shell.execute_reply": "2024-04-26T12:27:03.150534Z"
    },
    "trusted": true
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "###To work with",
   "metadata": {
    "id": "KCt4S6yO7Emh"
   }
  },
  {
   "cell_type": "code",
   "source": "wandb.login()",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VgRbaiHxtbLX",
    "outputId": "71a4ffa7-60c0-43e1-96ed-064f27dc8ed5",
    "ExecuteTime": {
     "end_time": "2024-04-02T19:36:32.549569Z",
     "start_time": "2024-04-02T19:36:32.434568300Z"
    },
    "execution": {
     "iopub.status.busy": "2024-04-26T12:27:05.634723Z",
     "iopub.execute_input": "2024-04-26T12:27:05.635090Z",
     "iopub.status.idle": "2024-04-26T12:27:09.841847Z",
     "shell.execute_reply.started": "2024-04-26T12:27:05.635060Z",
     "shell.execute_reply": "2024-04-26T12:27:09.840955Z"
    },
    "trusted": true
   },
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "text": "\u001B[34m\u001B[1mwandb\u001B[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001B[34m\u001B[1mwandb\u001B[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001B[34m\u001B[1mwandb\u001B[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:",
     "output_type": "stream"
    },
    {
     "output_type": "stream",
     "name": "stdin",
     "text": "  ········································\n"
    },
    {
     "name": "stderr",
     "text": "\u001B[34m\u001B[1mwandb\u001B[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
     "output_type": "stream"
    },
    {
     "execution_count": 4,
     "output_type": "execute_result",
     "data": {
      "text/plain": "True"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "wandb.init(project='Test1', config={\n    'learning_rate': 1e-3,\n    'margin': 0.2,\n    'net': 'TimmMobileNet large',\n    'optimizer': 'AdamW',\n    'train dataset': 'DenseUAV',\n    'test dataset': 'DenseUAV',\n    'miner': 'semi-hard samples'\n})",
   "metadata": {
    "id": "eNfYFpGWuy-5",
    "ExecuteTime": {
     "end_time": "2024-04-02T19:36:55.497120600Z",
     "start_time": "2024-04-02T19:36:39.897239Z"
    },
    "execution": {
     "iopub.status.busy": "2024-04-26T12:28:54.792814Z",
     "iopub.execute_input": "2024-04-26T12:28:54.793582Z",
     "iopub.status.idle": "2024-04-26T12:29:26.666426Z",
     "shell.execute_reply.started": "2024-04-26T12:28:54.793534Z",
     "shell.execute_reply": "2024-04-26T12:29:26.665568Z"
    },
    "trusted": true
   },
   "execution_count": 21,
   "outputs": [
    {
     "name": "stderr",
     "text": "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mmc1granec2003\u001B[0m (\u001B[33mdiploma_work\u001B[0m). Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "wandb version 0.16.6 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.16.4"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>/kaggle/working/wandb/run-20240426_122854-feze0188</code>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href='https://wandb.ai/diploma_work/Test1/runs/feze0188' target=\"_blank\">autumn-rain-4</a></strong> to <a href='https://wandb.ai/diploma_work/Test1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View project at <a href='https://wandb.ai/diploma_work/Test1' target=\"_blank\">https://wandb.ai/diploma_work/Test1</a>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run at <a href='https://wandb.ai/diploma_work/Test1/runs/feze0188' target=\"_blank\">https://wandb.ai/diploma_work/Test1/runs/feze0188</a>"
     },
     "metadata": {}
    },
    {
     "execution_count": 21,
     "output_type": "execute_result",
     "data": {
      "text/html": "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/diploma_work/Test1/runs/feze0188?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>",
      "text/plain": "<wandb.sdk.wandb_run.Run at 0x7b11dd005870>"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": "GEOMETRIC_DOUBLE = 'GEOMETRIC_DOUBLE'\nGEOMETRIC_SINGLE = 'GEOMETRIC_SINGLE'\nFINE_SINGLE = 'FINE_SINGLE'\nCOLOR_DOUBLE = 'COLOR_DOUBLE'\nCOLOR_SINGLE = 'COLOR_SINGLE'\nRANDOM_CROP_SINGLE = 'RANDOM_CROP_SINGLE'\nRANDOM_CROP_DOUBLE = 'RANDOM_CROP_DOUBLE'\n\ndef make_train_aug(size=(512, 512)):\n    h, w = size\n    geometric_aug = [\n        A.Flip(p=0.75),\n        A.Transpose(p=0.5),\n        A.RandomRotate90(p=0.75),\n        # A.ShiftScaleRotate(scale_limit=(-0.5, 0.5), shift_limit=0, rotate_limit=45, p=0.9),\n        A.Perspective(p=0.25),\n        A.PadIfNeeded(min_height=h, min_width=w, always_apply=True, border_mode=0),\n        ]\n\n    geometric_double= A.Compose(geometric_aug, additional_targets={'positive': 'image'})\n\n    color_aug = [\n        A.Sharpen (alpha=(0.05, 0.1), lightness=(0.1, 0.5), p=0.5),\n        A.RandomBrightnessContrast(brightness_limit=0.05, contrast_limit=0.05, p=1.0),\n        A.RGBShift(),\n        A.CLAHE(p=0.2),\n        A.RandomGamma(p=1),\n        A.HueSaturationValue(p=1),\n        A.ChannelShuffle(p=0.2),\n\n        A.OneOf([\n            A.GaussNoise(p=1),\n            A.Emboss(p=1),\n            A.Sharpen(p=1),\n            A.ImageCompression(p=1),\n        ], p=0.75),\n\n        A.OneOf([\n            A.Blur(blur_limit=3, p=1),\n            A.GaussianBlur(blur_limit=3, p=1),\n            A.MedianBlur(blur_limit=3, p=1),\n            A.MotionBlur(blur_limit=3, p=1),\n        ], p=0.75),\n        ]\n\n    color_double = A.Compose(color_aug, additional_targets={'negative': 'image'})\n\n    return {\n        GEOMETRIC_DOUBLE: geometric_double,\n        COLOR_DOUBLE: color_double,\n        }",
   "metadata": {
    "id": "zdB7Xe0TqcLY",
    "ExecuteTime": {
     "end_time": "2024-04-02T19:37:07.431642400Z",
     "start_time": "2024-04-02T19:37:05.904133100Z"
    },
    "execution": {
     "iopub.status.busy": "2024-04-26T12:29:41.952675Z",
     "iopub.execute_input": "2024-04-26T12:29:41.953624Z",
     "iopub.status.idle": "2024-04-26T12:29:41.968376Z",
     "shell.execute_reply.started": "2024-04-26T12:29:41.953578Z",
     "shell.execute_reply": "2024-04-26T12:29:41.967390Z"
    },
    "trusted": true
   },
   "execution_count": 23,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "from geopy.distance import geodesic\n\ndef parse_coordinates(line):\n    parts = line.split()\n    easting = float(parts[1][1:])\n    northing = float(parts[2][1:])\n    return easting, northing\n\ndef calculate_distance_between_coordinates(file_path, indexes_true, indexes_pred):\n    distance = 0\n    \n    with open(file_path, 'r') as file:\n        lines = file.readlines()\n        for index_true, index_pred in zip(indexes_true, indexes_pred):\n            \n            line_true = lines[index_true].strip()\n            line_pred = lines[index_pred].strip()\n\n            lon_true, lat_true = parse_coordinates(line_true)\n            lon_pred, lat_pred = parse_coordinates(line_pred)\n\n            distance += geodesic((lat_true, lon_true), (lat_pred, lon_pred)).meters\n            \n        return distance / indexes_true.shape[0]\n\ngeo_data_path = \"/kaggle/input/dataset/DenseUAV/Dense_GPS_test.txt\"",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-26T12:27:18.438386Z",
     "iopub.execute_input": "2024-04-26T12:27:18.439268Z",
     "iopub.status.idle": "2024-04-26T12:27:18.609573Z",
     "shell.execute_reply.started": "2024-04-26T12:27:18.439225Z",
     "shell.execute_reply": "2024-04-26T12:27:18.608628Z"
    },
    "trusted": true
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "class Mode(Enum):\n  TRAIN = 1\n  VAL = 2\n\nclass DenseUAVDataset(Dataset):\n  def __init__(self, root, mode):\n    self.root = root\n    self.mode = mode\n    self.is_debug = False\n\n    if self.mode == Mode.VAL:\n      self.root += '/test/'\n      self.query_path = 'query_drone/'\n      self.ref_path = 'gallery_satellite/'\n    else:\n      self.root += '/train/'\n      self.query_path = 'drone/'\n      self.ref_path = 'satellite/'\n\n    sorted(os.listdir(self.root + self.query_path))\n    self.query_folders = sorted(os.listdir(self.root + self.query_path))\n    self.ref_folders = sorted(os.listdir(self.root + self.ref_path))\n\n    self.query_img_path = []\n    self.ref_img_path = []\n\n    if self.mode == Mode.TRAIN:\n        \n        for folder_name in self.query_folders:\n            folder_path = os.path.join(self.root + self.query_path, folder_name)\n            self.query_img_path.extend([\n                    os.path.join(folder_path, f\"H100.JPG\"),\n                    os.path.join(folder_path, f\"H90.JPG\"),\n                    os.path.join(folder_path, f\"H80.JPG\")\n                ])\n\n        for folder_name in self.ref_folders:\n            folder_path = os.path.join(self.root + self.ref_path, folder_name)\n            self.ref_img_path.extend([\n                    os.path.join(folder_path, f\"H100_old.tif\") if random.randint(0, 1) == 0 else os.path.join(folder_path, f\"H100_old.tif\"),\n                    os.path.join(folder_path, f\"H90_old.tif\") if random.randint(0, 1) == 0 else os.path.join(folder_path, f\"H90_old.tif\"),\n                    os.path.join(folder_path, f\"H80.tif\") if random.randint(0, 1) == 0 else os.path.join(folder_path, f\"H80_old.tif\")\n                ])\n    \n    else:\n        \n        self.ref_folders = sorted(os.listdir(self.root + self.ref_path))[int(self.query_folders[0]):]\n        \n        for folder_name in self.query_folders:\n          folder_path = os.path.join(self.root + self.query_path, folder_name)\n          self.query_img_path.append(os.path.join(folder_path, f\"H100.JPG\"))\n\n        for folder_name in self.ref_folders:\n          folder_path = os.path.join(self.root + self.ref_path, folder_name)\n          self.ref_img_path.append(os.path.join(folder_path, f\"H100.tif\"))\n\n\n  def get_image(self, path):\n    image = Image.open(path)\n\n    return image\n\n  def generate_ref_image(self, index):\n    positive_folder = self.ref_img_path[index]\n    rand_idx = random.randint(0, len(self.ref_img_path) - 1) # 0, 1, 2 - 3, 4 ... +  3, 4, 5 - 0, 1, 2, 6, 7 , 8\n    valid_range_start = (index // 3) * 3\n    valid_range_end = valid_range_start + 2\n    while rand_idx > valid_range_start and rand_idx < valid_range_end: #3826 1289\n      rand_idx = random.randint(0, len(self.ref_img_path) - 1)\n    negative_folder = self.ref_img_path[rand_idx]\n    return positive_folder, negative_folder, torch.as_tensor(rand_idx // 3, dtype=torch.int)\n\n\n  def apply_color_transfer(self, image_target, image_source):\n    mean_target, mean_source = np.mean(image_target, axis=(0, 1)), np.mean(image_source, axis=(0, 1))\n    std_target, std_source = np.std(image_target, axis=(0, 1)), np.std(image_source, axis=(0, 1))\n\n    colored_image = (image_source - mean_source) * (std_target / std_source) + mean_target\n    colored_image = np.clip(colored_image, 0, 255).astype(np.uint8)\n    return colored_image\n\n  def custom_transform(self, image, is_ref=False):\n\n    image = transforms.ToTensor()(image)\n    image = transforms.Resize((224, 224))(image)\n    image = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])(image)\n\n    return image\n\n\n  def __getitem__(self, index):\n    anchor_label, positive_label = torch.as_tensor(index // 3, dtype=torch.int), torch.as_tensor(index // 3, dtype=torch.int)\n    anchor = self.get_image(self.query_img_path[index])\n    positive_path, negative_path, negative_label = self.generate_ref_image(index)\n\n    positive = self.get_image(positive_path)\n    negative = self.get_image(negative_path)\n\n    if self.mode == Mode.TRAIN:\n      sample = make_train_aug()[GEOMETRIC_DOUBLE](image=cv.resize(np.array(anchor), (512, 512)), positive=np.array(positive))\n      anchor, positive = sample['image'], sample['positive']\n\n      sample = make_train_aug()[COLOR_DOUBLE](image=np.array(positive), negative=np.array(negative))\n      positive, negative = sample['image'], sample['negative']\n\n    if not self.is_debug:\n        anchor = self.custom_transform(anchor)\n        positive = self.custom_transform(positive, is_ref=True)\n        negative = self.custom_transform(negative, is_ref=True)\n\n    return anchor, positive, negative, anchor_label, positive_label, negative_label\n\n\n  def __len__(self):\n    return len(self.query_img_path)\n",
   "metadata": {
    "id": "YvqPWAYTNkZE",
    "ExecuteTime": {
     "end_time": "2024-04-02T19:37:19.361177900Z",
     "start_time": "2024-04-02T19:37:19.190177Z"
    },
    "execution": {
     "iopub.status.busy": "2024-04-26T12:27:22.683356Z",
     "iopub.execute_input": "2024-04-26T12:27:22.684204Z",
     "iopub.status.idle": "2024-04-26T12:27:22.710752Z",
     "shell.execute_reply.started": "2024-04-26T12:27:22.684167Z",
     "shell.execute_reply": "2024-04-26T12:27:22.709715Z"
    },
    "trusted": true
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "train_data = DenseUAVDataset('/kaggle/input/dataset/DenseUAV', mode=Mode.TRAIN)\nval_data = DenseUAVDataset('/kaggle/input/dataset/DenseUAV', mode=Mode.VAL)",
   "metadata": {
    "id": "_f3e6SdvOj2L",
    "ExecuteTime": {
     "end_time": "2024-04-02T19:38:40.359204300Z",
     "start_time": "2024-04-02T19:38:39.244430900Z"
    },
    "execution": {
     "iopub.status.busy": "2024-04-26T12:27:26.779088Z",
     "iopub.execute_input": "2024-04-26T12:27:26.779850Z",
     "iopub.status.idle": "2024-04-26T12:27:28.119267Z",
     "shell.execute_reply.started": "2024-04-26T12:27:26.779814Z",
     "shell.execute_reply": "2024-04-26T12:27:28.118482Z"
    },
    "trusted": true
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "train_dataloader1 = DataLoader(train_data, batch_size=32, shuffle=False, num_workers=0, pin_memory=True)\ntrain_dataloader2 = DataLoader(train_data, batch_size=32, shuffle=True, num_workers=0, pin_memory=True)\nval_dataloader = DataLoader(val_data, batch_size=777, shuffle=False, num_workers=0, pin_memory=True)",
   "metadata": {
    "id": "GjDAEBh4wu0J",
    "ExecuteTime": {
     "end_time": "2024-04-02T19:41:00.027535700Z",
     "start_time": "2024-04-02T19:40:58.492983100Z"
    },
    "execution": {
     "iopub.status.busy": "2024-04-26T13:46:42.788052Z",
     "iopub.execute_input": "2024-04-26T13:46:42.788445Z",
     "iopub.status.idle": "2024-04-26T13:46:42.796332Z",
     "shell.execute_reply.started": "2024-04-26T13:46:42.788413Z",
     "shell.execute_reply": "2024-04-26T13:46:42.795279Z"
    },
    "trusted": true
   },
   "execution_count": 32,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "class TimmMobilenet(torch.nn.Module):\n    def __init__(self, timm_name='mobilenetv3_large_100'):\n        super(TimmMobilenet, self).__init__()\n        self.source_model = timm.create_model(timm_name, pretrained=True)\n\n    def forward(self, x):\n        x = self.source_model.forward_features(x)\n        x = self.source_model.global_pool(x)\n        x = self.source_model.conv_head(x)\n        x = self.source_model.act2(x)\n\n        x = torch.flatten(x, start_dim=1)\n        x = F.normalize(x, p=2, dim=1)\n\n        return x",
   "metadata": {
    "id": "Bux32BLpcJ8-",
    "ExecuteTime": {
     "end_time": "2024-04-02T19:38:59.044308300Z",
     "start_time": "2024-04-02T19:38:58.870317200Z"
    },
    "execution": {
     "iopub.status.busy": "2024-04-26T12:27:32.213894Z",
     "iopub.execute_input": "2024-04-26T12:27:32.214522Z",
     "iopub.status.idle": "2024-04-26T12:27:32.221190Z",
     "shell.execute_reply.started": "2024-04-26T12:27:32.214477Z",
     "shell.execute_reply": "2024-04-26T12:27:32.220147Z"
    },
    "trusted": true
   },
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "device = 'cuda'\n\nmodel = TimmMobilenet().to(device)\n\ncriterion = torch.nn.TripletMarginLoss(margin=0.2)\n\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-5)\n\n# with wandb.restore('epoch_33.pth', run_path='diploma_work/Test1/uz7rwknw') as io:\n#   name = io.name\n\n# model.load_state_dict(torch.load(name, map_location=device))",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xh0MRka09Ylb",
    "outputId": "d67723a2-410e-480d-c24d-9613c40eabb9",
    "ExecuteTime": {
     "end_time": "2024-04-02T19:44:03.057935100Z",
     "start_time": "2024-04-02T19:44:01.577291500Z"
    },
    "execution": {
     "iopub.status.busy": "2024-04-26T12:27:40.021147Z",
     "iopub.execute_input": "2024-04-26T12:27:40.021521Z",
     "iopub.status.idle": "2024-04-26T12:27:41.051761Z",
     "shell.execute_reply.started": "2024-04-26T12:27:40.021482Z",
     "shell.execute_reply": "2024-04-26T12:27:41.051000Z"
    },
    "trusted": true
   },
   "execution_count": 10,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "model.safetensors:   0%|          | 0.00/22.1M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "312e0386980345c58b9d7224174ca036"
      }
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from typing import List\n",
    "\n",
    "def accuracy(dists: torch.Tensor, labels: List[int], top_k=(1,), samples_per_class=3) -> List[torch.FloatTensor]:\n",
    "    maxk = max(top_k)\n",
    "    batch_size = dists.size(0)\n",
    "\n",
    "    #y_pred = torch.argsort(dists, dim=1) // samples_per_class\n",
    "    _, y_pred = dists.topk(k=maxk, dim=1, largest=False)# [B, n_classes] -> [B, maxk]\n",
    "    y_pred = y_pred.t() #// samples_per_class # [B, maxk] -> [maxk, B]\n",
    "\n",
    "    labels_reshaped = labels.view(1, -1).expand_as(y_pred) # B -> [1, B] -> [maxk, B]\n",
    "    correct = (y_pred == labels_reshaped)\n",
    "    list_topk_accs = []\n",
    "    for k in top_k:\n",
    "        ind_which_topk_matched_truth = correct[:k]  # [maxk, B] -> [k, B]\n",
    "        flattened_indicator_which_topk_matched_truth = ind_which_topk_matched_truth.reshape(-1).float()  # [k, B] -> [k * B]\n",
    "        #top_correct_topk = torch.any(ind_which_topk_matched_truth, dim=0).float().sum(dim=0, keepdim=True)\n",
    "        top_correct_topk = flattened_indicator_which_topk_matched_truth.float().sum(dim=0, keepdim=True)  # [k * B] -> [1]\n",
    "        topk_acc = top_correct_topk / batch_size\n",
    "        list_topk_accs.append(topk_acc)\n",
    "\n",
    "    return torch.stack(list_topk_accs).reshape(-1)"
   ],
   "metadata": {
    "id": "RJPtTrl2z8nX",
    "ExecuteTime": {
     "end_time": "2024-04-02T19:39:16.467311200Z",
     "start_time": "2024-04-02T19:39:16.282319Z"
    },
    "execution": {
     "iopub.status.busy": "2024-04-26T12:27:43.664237Z",
     "iopub.execute_input": "2024-04-26T12:27:43.664608Z",
     "iopub.status.idle": "2024-04-26T12:27:43.673534Z",
     "shell.execute_reply.started": "2024-04-26T12:27:43.664576Z",
     "shell.execute_reply": "2024-04-26T12:27:43.672297Z"
    },
    "trusted": true
   },
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from pytorch_metric_learning.losses import BaseMetricLossFunction\n",
    "from pytorch_metric_learning.reducers import AvgNonZeroReducer\n",
    "from pytorch_metric_learning.utils import common_functions as c_f\n",
    "from pytorch_metric_learning.utils import loss_and_miner_utils as lmu\n",
    "# taken from \n",
    "# https://github.com/KevinMusgrave/pytorch-metric-learning/blob/master/src/pytorch_metric_learning/losses/triplet_margin_loss.py\n",
    "# to slightly modify smooth_loss\n",
    "class TripletMarginLoss(BaseMetricLossFunction):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        margin=0.2,\n",
    "        swap=False,\n",
    "        smooth_loss=True,\n",
    "        triplets_per_anchor=\"all\",\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.margin = margin\n",
    "        self.swap = swap\n",
    "        self.smooth_loss = smooth_loss\n",
    "        self.triplets_per_anchor = triplets_per_anchor\n",
    "        self.add_to_recordable_attributes(list_of_names=[\"margin\"], is_stat=False)\n",
    "\n",
    "    def compute_loss(self, embeddings, labels, indices_tuple, ref_emb, ref_labels):\n",
    "        c_f.labels_or_indices_tuple_required(labels, indices_tuple)\n",
    "        indices_tuple = lmu.convert_to_triplets(\n",
    "            indices_tuple, labels, ref_labels, t_per_anchor=self.triplets_per_anchor\n",
    "        )\n",
    "        anchor_idx, positive_idx, negative_idx = indices_tuple\n",
    "        if len(anchor_idx) == 0:\n",
    "            return self.zero_losses()\n",
    "        mat = self.distance(embeddings, ref_emb)\n",
    "        ap_dists = mat[anchor_idx, positive_idx]\n",
    "        an_dists = mat[anchor_idx, negative_idx]\n",
    "        if self.swap:\n",
    "            pn_dists = mat[positive_idx, negative_idx]\n",
    "            an_dists = self.distance.smallest_dist(an_dists, pn_dists)\n",
    "\n",
    "        current_margins = self.distance.margin(ap_dists, an_dists)\n",
    "        violation = current_margins + self.margin\n",
    "        if self.smooth_loss:\n",
    "            loss = torch.nn.functional.softplus(violation,beta=3)\n",
    "        else:\n",
    "            loss = torch.nn.functional.relu(violation)\n",
    "\n",
    "        return {\n",
    "            \"loss\": {\n",
    "                \"losses\": loss,\n",
    "                \"indices\": indices_tuple,\n",
    "                \"reduction_type\": \"triplet\",\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def get_default_reducer(self):\n",
    "        return AvgNonZeroReducer()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-04-26T12:27:49.326299Z",
     "iopub.execute_input": "2024-04-26T12:27:49.326933Z",
     "iopub.status.idle": "2024-04-26T12:27:49.339136Z",
     "shell.execute_reply.started": "2024-04-26T12:27:49.326897Z",
     "shell.execute_reply": "2024-04-26T12:27:49.338159Z"
    },
    "trusted": true
   },
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "def run(train_dataloader, val_dataloader, model, criterion, optimizer, save_path, \n        num_epochs=50, samples_per_class=3, log_freq=26):\n    \n   def proccess_batch(batch, train):\n    anchor, positive, negative, anchor_label, positive_label, negative_label = batch\n    anchor, positive, negative, anchor_label, positive_label, negative_label = anchor.to(device), positive.to(device), negative.to(device), anchor_label.to(device), positive_label.to(device), negative_label.to(device)\n\n    if train:\n      optimizer.zero_grad()\n\n    anchor = model.forward(anchor)      # b * output_size\n\n    positive = model.forward(positive)  # b * output_size\n    negative = model.forward(negative)  # b * output_size\n\n    ref_embeddings = positive\n\n    ref_labels = positive_label\n\n    anchor_to_positive = F.pairwise_distance(anchor, positive) #b\n    anchor_to_negative = F.pairwise_distance(anchor, negative) #b\n\n    valid_distances = torch.sum(anchor_to_positive < anchor_to_negative)\n    acc = valid_distances / anchor_to_positive.shape[0]\n    default_loss = criterion(anchor, positive, negative)\n\n    miner_func = miners.TripletMarginMiner(margin=0.2, type_of_triplets=\"hard\")\n\n    miner_output = miner_func(anchor, anchor_label, ref_embeddings, ref_labels)\n    loss_func = TripletMarginLoss(margin=0.2)\n    \n    loss = loss_func(anchor, anchor_label, miner_output, ref_embeddings, ref_labels)\n        \n    if train:\n      loss.backward()\n      optimizer.step()\n\n    return loss, default_loss, acc, miner_func\n\n   wandb.watch(model, log_freq=log_freq)\n\n   for epoch in range(num_epochs):\n        print(f'EPOCH #{epoch + 1} ---------------')\n        train_miner_loss, train_default_loss, train_acc, train_mined_samples, loss0_cnt = 0, 0, 0, 0, 0\n        model.train()\n        for idx, (batch1, batch2) in enumerate(zip(train_dataloader1, train_dataloader2)):\n          concatenated_batch = []\n          for tensor1, tensor2 in zip(batch1, batch2):\n                concatenated_tensor = torch.cat((tensor1, tensor2), dim=0)\n                concatenated_batch.append(concatenated_tensor)\n          miner_loss, default_loss, acc, miner_func = proccess_batch(concatenated_batch, train=True)\n          train_miner_loss += float(miner_loss)\n          if float(miner_loss) == 0.0:\n            loss0_cnt += 1\n          train_default_loss += float(default_loss)\n          train_mined_samples += miner_func.num_triplets\n          train_acc += acc\n          if idx % log_freq == 0:\n            print(\"Train\\nStep #{}, miner_loss: {}, default_loss: {}, acc: {}\\n\".format((idx + 1) // log_freq, miner_loss.item(), default_loss.item(), acc))\n            print(\"Triplets mined: {}\".format(miner_func.num_triplets))\n        \n        torch.save(model.state_dict(), f'epoch_{epoch + 1}.pth')\n\n        val_miner_loss, val_default_loss, val_acc, val_mined_samples = 0, 0, 0, 0\n        model.eval()\n        for idx, batch in enumerate(val_dataloader):\n            with torch.no_grad():\n                miner_loss, default_loss, acc, miner_func = proccess_batch(batch, train=False)\n                val_miner_loss += float(miner_loss)\n                val_default_loss += float(default_loss)\n                val_mined_samples += miner_func.num_triplets\n                val_acc += acc\n            if idx % log_freq == 0:\n                print(\"Test\\nStep #{}, miner_loss: {}, default_loss: {}, acc: {}\\n\".format((idx + 1) // log_freq, miner_loss.item(), default_loss.item(), acc))\n                print(\"Triplets mined: {}\".format(miner_func.num_triplets))\n                \n        print('Avg train miner loss: ', train_miner_loss / (len(train_dataloader) - loss0_cnt))\n        print('Avg val miner loss: ', val_miner_loss / len(val_dataloader))\n\n        top_k = (1, 5, 10)\n        topk_accuracy = 0\n        distance = 0\n        \n        for idx, batch in enumerate(val_dataloader):\n\n            query, ref, _, _, _, _ = batch\n\n            query, ref = query.to(device), ref.to(device)\n\n            with torch.no_grad():\n                query_output = model(query)\n                ref_output = model(ref)\n                dist = torch.cdist(query_output, ref_output) # b\n                \n            indexes_true = torch.arange(query.shape[0], device=device)\n            indexes_pred = torch.argsort(dist, dim=1)[:, 0] \n            \n            topk_accuracy += accuracy(dist, indexes_true, top_k=top_k)\n            print(accuracy(dist, indexes_true, top_k=top_k))\n            distance += calculate_distance_between_coordinates(geo_data_path, indexes_true // samples_per_class, indexes_pred // samples_per_class)\n            \n        print('topk: ', topk_accuracy / len(val_dataloader))\n        print('distance error:', distance / len(val_dataloader))\n        \n        wandb.save(f'epoch_{epoch + 1}.pth')\n        wandb.log({\"train_loss\": train_default_loss / len(train_dataloader),\n                   \"val_loss\":val_default_loss / len(val_dataloader),\n                   \"train_miner_loss\": train_miner_loss / len(train_dataloader),\n                   \"val_miner_loss\": val_miner_loss / len(val_dataloader),\n                   \"train_acc\": train_acc / len(train_dataloader),\n                   \"val_acc\":val_acc / len(val_dataloader),\n                   \"top1\": topk_accuracy[0] / len(val_dataloader),\n                   \"top5\": topk_accuracy[1] / len(val_dataloader),\n                   \"top10\": topk_accuracy[2] / len(val_dataloader),\n                   \"distance error\": distance / len(val_dataloader),\n                   \"avg_train_mined_samples\": train_mined_samples / len(train_dataloader),\n                   \"avg_val_mined_samples\": val_mined_samples / len(val_dataloader)})\n\n\n",
   "metadata": {
    "id": "BW2PeX2n9VhY",
    "ExecuteTime": {
     "end_time": "2024-04-02T19:39:46.691311200Z",
     "start_time": "2024-04-02T19:39:46.490317800Z"
    },
    "execution": {
     "iopub.status.busy": "2024-04-26T12:34:23.023857Z",
     "iopub.execute_input": "2024-04-26T12:34:23.024234Z",
     "iopub.status.idle": "2024-04-26T12:34:23.052785Z",
     "shell.execute_reply.started": "2024-04-26T12:34:23.024208Z",
     "shell.execute_reply": "2024-04-26T12:34:23.051767Z"
    },
    "trusted": true
   },
   "execution_count": 30,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "run(train_dataloader1, val_dataloader, model, criterion, optimizer, 'Models/timm_model')",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d88seWcDqYOa",
    "outputId": "fccb7fdd-3359-4cc1-8302-7644f0965834",
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-04-02T19:44:19.424975300Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
